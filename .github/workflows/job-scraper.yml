name: Job Scraper (8 Hour Schedule)

on:
  schedule:
    - cron: '0 */8 * * *'  # Runs every 8 hours
  workflow_dispatch:      # Allows manual trigger

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    
    env:
      SENDER_EMAIL: ${{ secrets.SENDER_EMAIL }}
      SENDER_PASSWORD: ${{ secrets.SENDER_PASSWORD }}
      RECEIVER_EMAIL: ${{ secrets.RECEIVER_EMAIL }}
      SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
      GOOGLE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_CREDENTIALS_JSON }}

    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run job scraper
      run: python main.py
      
    - name: Deploy to GitHub Pages
      if: success()
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./portfolio

    - name: Upload artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: scraping-results
        path: |
          jobs.csv
          portfolio/